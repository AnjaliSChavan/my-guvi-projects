# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wFp3hpMLvHAGpuDqaaGm5Y62nS0qiKqt
"""



# Commented out IPython magic to ensure Python compatibility.
# General imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

# Words analysis oriented imports

# Natural Language ToolKit
import nltk
# If you uncomment, it will launch an installation popup. Go to the "Models" tab and select "punkt" from the "Identifier" column. Then click "Download" and it will install the necessary files.
#nltk.download()
from nltk.stem import PorterStemmer # For words normalization
from sklearn.feature_extraction.text import CountVectorizer # For words frequency
from sklearn.feature_extraction.text import TfidfTransformer # For words weighting

# Graphics
# %matplotlib inline
import matplotlib.pyplot as plt

# Wordcloud
from wordcloud import WordCloud
import seaborn as sns

df = pd.read_csv("twitter_new.csv", header=None, encoding = "ISO-8859-1")

df.head()

cols = ['sentiment_score', 'id', 'date', 'query', 'author', 'tweet']

# Rename the columns
df.columns = cols

# Convert all tweets in lowercase
df['tweet'] = df.tweet.map(lambda x: x.lower())

# Replace all the @username by the token 'USERNAME'
df.replace('\@\w+', 'USERNAME', regex=True, inplace=True)

# Replace all the urls by the token 'URL'
df.replace(r'''(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'".,<>?«»“”‘’]))''', 'URL', regex=True, inplace=True)

# Replace any letter occurring more than two times in a row with two occurrences. Ex : huuuungry becomes huungry.
# It's important to keep two occurences as they need to stay separated from the words with one occurence, because the sentiment behind a word with multiple character occurences may be stronger (positive or negative)
df.replace(r'(.)\1{2,}', r'\1\1', regex=True, inplace=True)

# Remove punctuation
df['tweet'] = df.tweet.str.replace('[^\w\s]', '')

df.head()

nltk.download('punkt')

# Tokenizer
df['tweet'] = df['tweet'].apply(nltk.word_tokenize)

# Word stemming
# Normalize our text for all variations of words carry the same meaning, regardless of the tense
stemmer = PorterStemmer()
df['tweet'] = df['tweet'].apply(lambda x: [stemmer.stem(y) for y in x])

df.head()

# Convert the list of words into space-separated strings
df['tweet'] = df['tweet'].apply(lambda x: ' '.join(x))

# Transform the data into occurences
count_vect = CountVectorizer()
counts = count_vect.fit_transform(df['tweet'])

# Use of Term Frequency Inverse Document Frequency, more known as tf-idf
# The tf–idf value increases proportionally to the number of times a word appears
transformer = TfidfTransformer().fit(counts)
counts = transformer.transform(counts)

import seaborn as sns
sns.countplot(df['sentiment_score'], label="Count")
plt.show()

def orange_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):
    h = int(360.0 * 21.0 / 255.0)
    s = int(100.0 * 255.0 / 255.0)
    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)

    return "hsl({}, {}%, {}%)".format(h, s, l)

def blue_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):
    h = int(360.0 * 170.0 / 255.0)
    s = int(100.0 * 255.0 / 255.0)
    l = int(100.0 * float(random_state.randint(100, 160)) / 255.0)

    return "hsl({}, {}%, {}%)".format(h, s, l)

wordcloud = WordCloud(
    color_func = orange_color_func,
    background_color='white',
    width=2000,
    height=1000
).generate(str(df.loc[df['sentiment_score'] == 4]['tweet'].values))

fig = plt.figure(
    figsize=(16, 12)
)
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

wordcloud = WordCloud(
    color_func = blue_color_func,
    background_color='white',
    width=2000,
    height=1000
).generate(str(df.loc[df['sentiment_score'] == 0]['tweet'].values))

fig = plt.figure(
    figsize=(16, 12)
)
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

X_train, X_test, y_train, y_test = train_test_split(counts, df['sentiment_score'], test_size=0.2, random_state=42)

from sklearn.naive_bayes import MultinomialNB

# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited
mnb = MultinomialNB()
mnb.fit(X_train, y_train)

print('Accuracy of NB classifier on training set: {:.2f}'
     .format(mnb.score(X_train, y_train)))
print('Accuracy of NB classifier on test set: {:.2f}'
     .format(mnb.score(X_test, y_test)))

predicted = mnb.predict(X_test)

# Print the confusion matrix
print(confusion_matrix(y_test, predicted))

from sklearn.svm import LinearSVR

svm = LinearSVR()
svm.fit(X_train, y_train)
print('Accuracy of SVM classifier on training set: {:.2f}'
     .format(svm.score(X_train, y_train)))
print('Accuracy of SVM classifier on test set: {:.2f}'
     .format(svm.score(X_test, y_test)))

from sklearn.linear_model import SGDClassifier


svm = SGDClassifier(max_iter=1000, tol=1e-3)
svm.fit(X_train, y_train)
print('Accuracy of SVM classifier on training set: {:.2f}'
     .format(svm.score(X_train, y_train)))
print('Accuracy of SVM classifier on test set: {:.2f}'
     .format(svm.score(X_test, y_test)))

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(solver = 'lbfgs', max_iter = 1000)
logreg.fit(X_train, y_train)

print('Accuracy of Logistic regression classifier on training set: {:.2f}'
     .format(logreg.score(X_train, y_train)))
print('Accuracy of Logistic regression classifier on test set: {:.2f}'
     .format(logreg.score(X_test, y_test)))









